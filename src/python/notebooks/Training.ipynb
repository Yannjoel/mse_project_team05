{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(0, \"..\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from joblib import dump\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "from RankingAlgorithms.neuralnetwork import NeuralNetwork\n",
    "from RankingAlgorithms.pwsvm import RankSVM\n",
    "from DataHandling.train_data import load_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_indices = [108, 18, 103, 3, 8, 13, 125, 126, 107, 17, 102, 2, 7, 12, 105, 15, 100, 0, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "X_train, y_train = load_data(path=\"../../../data/MSLR-WEB10K/Fold1/train.txt\", nrows=10000, feature_indices=feature_indices)\n",
    "#X_test, y_test = load_data(path=\"../../../data/MSLR-WEB10K/Fold1/test.txt\", nrows=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label counts (array([0., 1., 2., 3., 4.]), array([5481, 3000, 1326,  142,   51]))\n"
     ]
    }
   ],
   "source": [
    "print('label counts', np.unique(y_train, return_counts=True))\n",
    "n_samples_per_class = np.unique(y_train, return_counts=True)[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(255, 20) (255,)\n",
      "label counts:  (array([0., 1., 2., 3., 4.]), array([51, 51, 51, 51, 51]))\n"
     ]
    }
   ],
   "source": [
    "# balance dataset\n",
    "indices = []\n",
    "for label in range(5):\n",
    "    indices.append(list(np.random.choice(np.where(y_train == label)[0], n_samples_per_class, replace=False)))\n",
    "    \n",
    "indices = np.array(indices).flatten()\n",
    "\n",
    "X_cut = X_train[indices, :]\n",
    "y_cut = y_train[indices]\n",
    "\n",
    "print(X_cut.shape, y_cut.shape)\n",
    "print('label counts: ', np.unique(y_cut, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trans = torch.zeros(len(y_cut), 5)\n",
    "for i, label in enumerate(y_cut):\n",
    "    y_trans[i, 0:int(label)+1] = 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(n_features=len(feature_indices), n_hidden=10, load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define optimizer and loss fct\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 5.243661  [   32/  255]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 4.241916  [   32/  255]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 4.698521  [   32/  255]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 4.940804  [   32/  255]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 5.576783  [   32/  255]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 4.185291  [   32/  255]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 4.594983  [   32/  255]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 5.218893  [   32/  255]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 4.765697  [   32/  255]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 4.549974  [   32/  255]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 4.565063  [   32/  255]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 4.735998  [   32/  255]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 5.642962  [   32/  255]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 4.451110  [   32/  255]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 4.740330  [   32/  255]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 5.195291  [   32/  255]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 4.985036  [   32/  255]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 4.546423  [   32/  255]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 5.264417  [   32/  255]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 4.784495  [   32/  255]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 4.370317  [   32/  255]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 3.801475  [   32/  255]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 4.421402  [   32/  255]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 5.227326  [   32/  255]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 4.613298  [   32/  255]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 4.338768  [   32/  255]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 4.513495  [   32/  255]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 5.075068  [   32/  255]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 4.963473  [   32/  255]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 4.486143  [   32/  255]\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 4.651168  [   32/  255]\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 4.791027  [   32/  255]\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 4.303174  [   32/  255]\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 4.538228  [   32/  255]\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 4.868941  [   32/  255]\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 3.731192  [   32/  255]\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 4.946407  [   32/  255]\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 4.996327  [   32/  255]\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 4.427279  [   32/  255]\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 4.825509  [   32/  255]\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 5.095853  [   32/  255]\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 3.803602  [   32/  255]\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 4.596779  [   32/  255]\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 4.900244  [   32/  255]\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 4.324611  [   32/  255]\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 4.817069  [   32/  255]\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 4.793069  [   32/  255]\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 4.837004  [   32/  255]\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 4.356995  [   32/  255]\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 4.710345  [   32/  255]\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 4.548282  [   32/  255]\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 4.549256  [   32/  255]\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 4.179933  [   32/  255]\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 4.805603  [   32/  255]\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 4.912703  [   32/  255]\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 4.707407  [   32/  255]\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 4.451301  [   32/  255]\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 4.310349  [   32/  255]\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 4.899553  [   32/  255]\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 4.640939  [   32/  255]\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 4.561527  [   32/  255]\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 4.328631  [   32/  255]\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 4.580014  [   32/  255]\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 4.082583  [   32/  255]\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 4.691558  [   32/  255]\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 4.397262  [   32/  255]\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 5.053885  [   32/  255]\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 4.450047  [   32/  255]\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 4.277870  [   32/  255]\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 4.838393  [   32/  255]\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 4.275085  [   32/  255]\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 4.494780  [   32/  255]\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 4.531078  [   32/  255]\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 4.687704  [   32/  255]\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 4.386272  [   32/  255]\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 4.815433  [   32/  255]\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 4.427932  [   32/  255]\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 4.960585  [   32/  255]\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 4.425995  [   32/  255]\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 4.419046  [   32/  255]\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 4.740773  [   32/  255]\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 4.746563  [   32/  255]\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 4.836102  [   32/  255]\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 4.477072  [   32/  255]\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 4.251419  [   32/  255]\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 4.341627  [   32/  255]\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 4.425065  [   32/  255]\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 4.444168  [   32/  255]\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 4.936534  [   32/  255]\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 5.579843  [   32/  255]\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 3.971883  [   32/  255]\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 4.134728  [   32/  255]\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 4.371981  [   32/  255]\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 4.324981  [   32/  255]\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 5.061977  [   32/  255]\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 4.550084  [   32/  255]\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 4.394608  [   32/  255]\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 4.741398  [   32/  255]\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 4.980318  [   32/  255]\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 4.525541  [   32/  255]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loss = model.train_loop(torch.Tensor(X_cut), y_trans, loss, optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.2\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.evaluate(torch.Tensor(X_cut))\n",
    "print('accuracy: ', np.sum(np.array(y_pred) == np.array(y_cut)) / len(y_cut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True]), array([204,  51]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array(y_pred) == np.array(y_cut), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 3]), array([249,   6]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.unique(y_pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5951269310031864"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg = ndcg_score(y_cut.reshape(1, -1), y_pred.reshape(1, -1), k=20)\n",
    "ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save mdoel\n",
    "model.save(\"../models/nn.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise SVM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'src/python/models/ranksvm_coef.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m svm \u001b[39m=\u001b[39m RankSVM()\n\u001b[1;32m      2\u001b[0m svm\u001b[39m.\u001b[39mfit(X_cut, y_cut)\n",
      "File \u001b[0;32m~/Documents/Uni/MSE/mse_project_team05/src/python/notebooks/../RankingAlgorithms/pwsvm.py:53\u001b[0m, in \u001b[0;36mRankSVM.__init__\u001b[0;34m(self, penalty, loss, dual, tol, C, multi_class, fit_intercept, intercept_scaling, class_weight, verbose, random_state, max_iter, load)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39msuper\u001b[39m(RankSVM, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(penalty\u001b[39m=\u001b[39mpenalty, loss\u001b[39m=\u001b[39mloss, dual\u001b[39m=\u001b[39mdual, tol\u001b[39m=\u001b[39mtol, C\u001b[39m=\u001b[39mC, multi_class\u001b[39m=\u001b[39mmulti_class, fit_intercept\u001b[39m=\u001b[39mfit_intercept, intercept_scaling\u001b[39m=\u001b[39mintercept_scaling, class_weight\u001b[39m=\u001b[39mclass_weight, verbose\u001b[39m=\u001b[39mverbose, random_state\u001b[39m=\u001b[39mrandom_state, max_iter\u001b[39m=\u001b[39mmax_iter)\n\u001b[1;32m     52\u001b[0m \u001b[39mif\u001b[39;00m load:\n\u001b[0;32m---> 53\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_coef()\n",
      "File \u001b[0;32m~/Documents/Uni/MSE/mse_project_team05/src/python/notebooks/../RankingAlgorithms/pwsvm.py:103\u001b[0m, in \u001b[0;36mRankSVM.load_coef\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_coef\u001b[39m(\u001b[39mself\u001b[39m, path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msrc/python/models/ranksvm_coef.pkl\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_ \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39;49m(path, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'src/python/models/ranksvm_coef.pkl'"
     ]
    }
   ],
   "source": [
    "svm = RankSVM()\n",
    "svm.fit(X_cut, y_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.15770825,  0.50992615,  1.78252907,  5.5612664 , 14.27851976,\n",
       "        5.21922592,  2.59735256,  0.63082727,  2.38209859,  3.6481356 ,\n",
       "        1.24197684,  2.20891319,  1.04438219,  3.11336391,  6.63151552,\n",
       "        1.93513109,  3.39426069,  1.97573538, -0.02410986,  1.56432121,\n",
       "        0.67095663,  0.26377838,  1.68359401,  0.05318055,  1.34890445,\n",
       "        1.02213252,  1.67102159,  4.96835832,  4.10873211, 17.05088478,\n",
       "        3.12368885,  0.81831372,  5.05378742,  2.14668215,  4.9659123 ,\n",
       "        1.76079487,  4.06223585,  0.96345589,  3.47819071, 18.63590791,\n",
       "        1.05129043,  2.0381725 ,  5.86312733,  1.31376383,  3.92189547,\n",
       "        6.84027136,  4.41560397,  2.41779838,  0.3908853 ,  1.31975431,\n",
       "        1.89458781,  0.89293467,  1.37638772,  3.19030837,  1.843453  ,\n",
       "        0.939346  ,  1.79368408,  2.46226496,  3.07197132,  1.3375709 ,\n",
       "        1.22004651,  0.79768477,  1.50403245,  0.84332623, 10.92482543,\n",
       "        7.29285028,  2.28888023, -0.09932107,  3.06827776, -0.44777588,\n",
       "        1.24761195,  1.11105704,  0.87948349,  2.09210312,  1.61944911,\n",
       "        4.48901288,  1.63237854,  0.43731786,  4.19964527,  3.2909287 ,\n",
       "        3.33957078,  0.80434282,  0.61731366,  0.81797741, 17.11276054,\n",
       "        5.21565237,  1.85275392,  2.67602597,  1.63971895,  6.97682458,\n",
       "        1.24000322,  4.09118254,  3.41569471,  1.27114845,  0.31947011,\n",
       "       13.63319293,  1.08666148,  2.2749614 ,  1.07785006,  4.07524526,\n",
       "        2.76755392,  3.13369307,  2.58159617,  1.28983497,  0.23208794,\n",
       "        0.07886338,  2.11294817,  2.17178157,  0.45560368,  2.09754877,\n",
       "        2.66152642,  1.26888701,  0.86671738,  0.80270311,  2.72456281,\n",
       "        1.56825951,  2.39723098,  0.92064632,  2.05606284,  4.77984949,\n",
       "        2.42257683,  1.06334145,  4.16759303,  3.84477611,  1.69540343,\n",
       "        1.3314553 ,  2.87002104,  1.01047577,  2.08601643,  0.66745139,\n",
       "        2.14730364,  1.64315303,  0.07666218, -0.2204408 ,  1.73203033,\n",
       "        1.74349491,  3.70763343,  4.01446004,  0.89905806,  5.27855103,\n",
       "        4.84975173,  1.85918341,  4.26784746, 17.1954158 ,  4.93944106,\n",
       "        3.39389247,  6.38498046,  0.78774329,  6.98498107,  4.95495655,\n",
       "        1.56869618,  1.39754258,  0.82995107,  0.6112527 ,  1.82447844,\n",
       "        4.33044756,  1.31426391,  1.88781049,  1.42922776,  2.80459927,\n",
       "        0.32924075,  1.3540184 ,  0.85591812,  1.66892657,  0.19120451,\n",
       "        1.19733111,  1.11285901,  2.44146469,  9.84627485,  3.01381472,\n",
       "        1.29994583,  1.12723349,  0.60024218,  4.44865943,  1.83718355,\n",
       "        0.83457372,  2.71645034,  1.46181105,  0.46417916,  3.33336903,\n",
       "        0.27200584,  2.03119175,  2.33812786,  1.24421291,  2.20877087,\n",
       "        0.83706276,  2.02845325,  2.64561612,  1.27209817,  3.30807938,\n",
       "        3.69850088,  3.77338728,  2.88715275,  0.72555114,  0.45404997,\n",
       "        0.58859315,  4.42963577,  5.78902133,  2.55340786,  1.76723825,\n",
       "        1.63833328,  2.02481949,  0.83457372,  0.83273734,  3.03670276,\n",
       "        1.51244673,  1.40204425,  3.10345407,  1.45191528,  1.02537391,\n",
       "        0.67906539,  0.2688805 ,  1.21601122,  0.27609888,  0.84986039,\n",
       "        2.01330917,  0.77965831,  2.98657831,  1.72858848,  2.59134386,\n",
       "        1.43554793,  1.38741034,  1.8920492 ,  1.13732945,  1.65412867,\n",
       "        1.61954454,  2.25063714,  2.93706006,  2.06794769,  1.42693232,\n",
       "        1.89854792,  1.90657858,  1.21708466,  1.26843671,  3.2281082 ,\n",
       "        2.88408435,  1.16480986,  0.35554788,  0.75112936,  1.75733787,\n",
       "        3.10560352,  1.04821758,  2.05840834,  0.10575831,  1.69955825,\n",
       "        2.89734498,  2.11113741,  5.22790342,  1.7459878 ,  1.79736989,\n",
       "        2.92876335,  1.55507967,  1.6012967 ,  0.39113207,  3.06767492])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = svm.predict(X_cut)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23533060690920224"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg = ndcg_score(y_cut.reshape(1, -1), preds.reshape(1, -1), k=10)\n",
    "ndcg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mse_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
