{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(0, \"..\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from joblib import dump\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "from RankingAlgorithms.neuralnetwork import NeuralNetwork\n",
    "from RankingAlgorithms.pwsvm import RankSVM\n",
    "from DataHandling.train_data import load_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_indices = [108, 18, 103, 3, 8, 13, 125, 126, 107, 17, 102, 2, 7, 12, 105, 15, 100, 0, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "X_train, y_train = load_data(path=\"../../../data/MSLR-WEB10K/Fold1/train.txt\", nrows=10000, feature_indices=feature_indices)\n",
    "#X_test, y_test = load_data(path=\"../../../data/MSLR-WEB10K/Fold1/test.txt\", nrows=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label counts (array([0., 1., 2., 3., 4.]), array([5481, 3000, 1326,  142,   51]))\n"
     ]
    }
   ],
   "source": [
    "print('label counts', np.unique(y_train, return_counts=True))\n",
    "n_samples_per_class = np.unique(y_train, return_counts=True)[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(255, 20) (255,)\n",
      "label counts:  (array([0., 1., 2., 3., 4.]), array([51, 51, 51, 51, 51]))\n"
     ]
    }
   ],
   "source": [
    "# balance dataset\n",
    "indices = []\n",
    "for label in range(5):\n",
    "    indices.append(list(np.random.choice(np.where(y_train == label)[0], n_samples_per_class, replace=False)))\n",
    "    \n",
    "indices = np.array(indices).flatten()\n",
    "\n",
    "X_cut = X_train[indices, :]\n",
    "y_cut = y_train[indices]\n",
    "\n",
    "print(X_cut.shape, y_cut.shape)\n",
    "print('label counts: ', np.unique(y_cut, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trans = torch.zeros(len(y_cut), 5)\n",
    "for i, label in enumerate(y_cut):\n",
    "    y_trans[i, 0:int(label)+1] = 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(n_features=len(feature_indices), n_hidden=10, load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define optimizer and loss fct\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 5.646904  [   32/  255]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 5.230911  [   32/  255]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 5.024089  [   32/  255]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 4.659718  [   32/  255]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 4.626517  [   32/  255]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 4.831213  [   32/  255]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 5.546222  [   32/  255]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 4.679454  [   32/  255]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 4.542897  [   32/  255]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 4.620372  [   32/  255]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 4.130543  [   32/  255]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 4.545737  [   32/  255]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 4.987671  [   32/  255]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 5.100263  [   32/  255]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 4.678654  [   32/  255]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 4.874489  [   32/  255]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 4.057736  [   32/  255]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 4.191264  [   32/  255]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 4.681222  [   32/  255]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 4.001500  [   32/  255]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 4.930513  [   32/  255]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 4.673666  [   32/  255]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 5.326683  [   32/  255]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 4.589730  [   32/  255]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 4.732158  [   32/  255]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 4.445978  [   32/  255]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 5.215687  [   32/  255]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 4.633901  [   32/  255]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 4.582360  [   32/  255]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 4.564409  [   32/  255]\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 4.403944  [   32/  255]\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 4.643552  [   32/  255]\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 4.518227  [   32/  255]\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 5.683223  [   32/  255]\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 4.845644  [   32/  255]\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 4.798119  [   32/  255]\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 5.245863  [   32/  255]\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 4.823715  [   32/  255]\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 4.996169  [   32/  255]\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 4.873841  [   32/  255]\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 4.327458  [   32/  255]\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 4.531526  [   32/  255]\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 5.320378  [   32/  255]\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 4.042466  [   32/  255]\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 4.654372  [   32/  255]\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 4.601314  [   32/  255]\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 4.873965  [   32/  255]\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 4.771417  [   32/  255]\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 4.327315  [   32/  255]\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 5.115503  [   32/  255]\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 4.131093  [   32/  255]\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 4.994549  [   32/  255]\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 4.555590  [   32/  255]\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 4.585403  [   32/  255]\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 4.954151  [   32/  255]\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 3.903646  [   32/  255]\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 4.549268  [   32/  255]\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 5.005988  [   32/  255]\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 4.752196  [   32/  255]\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 4.783357  [   32/  255]\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 5.004413  [   32/  255]\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 4.963304  [   32/  255]\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 4.957335  [   32/  255]\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 5.236821  [   32/  255]\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 4.914104  [   32/  255]\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 5.123102  [   32/  255]\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 5.029820  [   32/  255]\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 4.790969  [   32/  255]\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 4.277004  [   32/  255]\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 4.611101  [   32/  255]\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 4.848570  [   32/  255]\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 4.681143  [   32/  255]\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 4.967371  [   32/  255]\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 4.793714  [   32/  255]\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 5.112606  [   32/  255]\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 4.713086  [   32/  255]\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 5.053515  [   32/  255]\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 4.717317  [   32/  255]\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 5.277736  [   32/  255]\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 4.827390  [   32/  255]\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 4.154971  [   32/  255]\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 4.629868  [   32/  255]\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 4.655168  [   32/  255]\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 4.592217  [   32/  255]\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 4.265840  [   32/  255]\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 5.225940  [   32/  255]\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 4.905049  [   32/  255]\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 4.539056  [   32/  255]\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 5.234172  [   32/  255]\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 4.407448  [   32/  255]\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 3.872079  [   32/  255]\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 4.741728  [   32/  255]\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 4.285521  [   32/  255]\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 4.254525  [   32/  255]\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 4.696465  [   32/  255]\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 4.723135  [   32/  255]\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 4.393607  [   32/  255]\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 4.350421  [   32/  255]\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 5.247541  [   32/  255]\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 4.850042  [   32/  255]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loss = model.train_loop(torch.Tensor(X_cut), y_trans, loss, optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.00784313725490196\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.evaluate(torch.Tensor(X_cut))\n",
    "print('accuracy: ', np.sum(np.array(y_pred) == np.array(y_cut)) / len(y_cut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True]), array([253,   2]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array(y_pred) == np.array(y_cut), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1]), array([239,  16]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.unique(y_pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5670634643155298"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg = ndcg_score(y_cut.reshape(1, -1), y_pred.reshape(1, -1), k=20)\n",
    "ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save mdoel\n",
    "model.save(\"../models/nn.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise SVM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annavollweiter/Documents/Uni/MSE/mse_project_team05/.venv/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/Users/annavollweiter/Documents/Uni/MSE/mse_project_team05/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../models/svm.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = RankSVM()\n",
    "svm.fit(X_cut, y_cut)\n",
    "dump(svm, \"../models/svm.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.90958881, 1.72799399, 1.86379818, 2.02755245, 2.99523792,\n",
       "       2.06847515, 1.7993963 , 1.50653503, 2.67641329, 1.82912804,\n",
       "       1.51366606, 1.38462541, 1.27792644, 1.83515976, 2.01484859,\n",
       "       1.76154178, 1.48537337, 1.47457715, 0.2519111 , 1.57014001,\n",
       "       1.39986249, 0.6585438 , 1.5304209 , 1.32594041, 1.727898  ,\n",
       "       1.026968  , 1.45968672, 1.88032662, 1.74742562, 2.15590273,\n",
       "       1.97467458, 1.51736082, 1.54048126, 1.56391523, 1.70900246,\n",
       "       1.57238727, 1.65967897, 2.3362331 , 1.34308825, 2.20015668,\n",
       "       1.64013029, 2.56013976, 2.53493189, 1.47565326, 2.04163444,\n",
       "       2.4037059 , 2.96781842, 2.29064057, 1.2449567 , 2.0794536 ,\n",
       "       1.27414051, 2.44241348, 2.27696806, 2.34336325, 1.5187813 ,\n",
       "       1.40743793, 2.6317349 , 2.12419614, 2.05946536, 1.40772719,\n",
       "       2.35468932, 1.83335929, 1.67964576, 1.58403716, 2.01716951,\n",
       "       2.3968    , 2.07763192, 1.52378332, 2.04193126, 1.90389818,\n",
       "       2.31289752, 1.38477485, 2.43500294, 1.90084897, 1.87890021,\n",
       "       2.68822283, 1.77171371, 1.79285155, 1.65810025, 2.36998742,\n",
       "       1.69897526, 2.45219201, 1.14502565, 1.38269521, 2.70026541,\n",
       "       2.80851293, 1.69630317, 1.4522485 , 1.31949896, 1.60692459,\n",
       "       1.93604872, 2.45309151, 1.77044209, 2.00338108, 1.96739694,\n",
       "       2.73578051, 1.20669438, 2.43452253, 1.4482864 , 2.37950815,\n",
       "       2.04681844, 1.91968672, 2.39211308, 1.87913857, 1.21215557,\n",
       "       2.79422919, 1.99593831, 2.74780854, 2.2104046 , 2.74927361,\n",
       "       2.1679346 , 1.85253411, 1.47980933, 1.49400283, 3.78562527,\n",
       "       1.42490071, 1.30346164, 1.37027882, 1.71455913, 2.54680981,\n",
       "       1.69621541, 1.76486947, 3.14931961, 2.03216187, 1.43132393,\n",
       "       2.63844596, 2.09865811, 1.61737412, 1.85417231, 1.32357592,\n",
       "       2.22365093, 1.7865952 , 1.25722057, 1.09212129, 1.33973911,\n",
       "       2.974351  , 2.52586213, 1.8343639 , 1.51230736, 2.46245273,\n",
       "       2.81470352, 1.75892083, 2.46390193, 2.64997625, 1.60397351,\n",
       "       2.23013045, 2.43525108, 1.29031875, 1.83870741, 2.0524972 ,\n",
       "       1.79920897, 1.05553019, 1.26496883, 2.31011531, 2.22105296,\n",
       "       2.08714326, 1.89620672, 1.44886844, 2.07533606, 1.77468521,\n",
       "       1.10702084, 1.6989247 , 3.79221617, 2.54924481, 1.67537133,\n",
       "       2.46373167, 1.83319736, 1.85210326, 2.20376862, 2.40720831,\n",
       "       2.00534718, 1.51620468, 1.08546992, 2.83979026, 1.63637999,\n",
       "       1.79452951, 2.37314761, 1.73333866, 2.57773569, 2.42628837,\n",
       "       1.46827149, 3.17318008, 1.40672132, 1.81215887, 1.5936644 ,\n",
       "       1.4996582 , 2.61971742, 1.50878617, 1.52120674, 1.77900488,\n",
       "       1.53357811, 2.57211123, 1.73776784, 1.91147488, 1.23675126,\n",
       "       1.0986363 , 2.34121296, 1.6395098 , 2.08455687, 1.42404875,\n",
       "       1.94569862, 2.01850406, 1.79452951, 2.37349526, 2.40378068,\n",
       "       2.00624214, 2.03675174, 3.35656604, 1.71498109, 2.37802093,\n",
       "       1.74938605, 1.58993547, 2.02878247, 3.11174002, 1.77798279,\n",
       "       3.49301392, 3.24073665, 3.33207751, 2.00281911, 2.44682607,\n",
       "       2.20602737, 2.35580005, 2.13445488, 1.64754412, 2.31964628,\n",
       "       1.94498294, 2.6430816 , 3.34781325, 2.1482149 , 2.05849291,\n",
       "       1.84382931, 3.47676095, 1.91653272, 2.62200049, 3.39078483,\n",
       "       2.56988114, 2.03732564, 2.15075576, 2.42505797, 2.06541668,\n",
       "       2.31574698, 1.73139695, 2.14793809, 1.61294536, 2.39897785,\n",
       "       2.08499627, 2.64493653, 2.25903086, 2.10815152, 1.75196126,\n",
       "       3.32772633, 1.66190884, 1.86054829, 0.96453294, 3.35458792])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = svm.predict(X_cut)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8755458364887027"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg = ndcg_score(y_cut.reshape(1, -1), preds.reshape(1, -1), k=10)\n",
    "ndcg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mse_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
