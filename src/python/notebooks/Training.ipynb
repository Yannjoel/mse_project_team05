{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/annavollweiter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(0, \"..\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from joblib import dump\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "from RankingAlgorithms.neuralnetwork import NeuralNetwork\n",
    "from RankingAlgorithms.pwsvm import RankSVM\n",
    "from DataHandling.train_data import load_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {'url_bm25': 108, 'url_idf': 18, 'url_vsm': 103,\n",
    "       'url_covered_query_term_number': 3, 'url_query_term_ratio': 8, 'url_stream_length': 13, 'url_n_slash':125, 'url_len_url': 126,\n",
    "       'title_bm25': 107, 'title_idf': 17, 'title_vsm': 102, \n",
    "       'title_covered_query_term_number': 2, 'title_query_term_ratio': 7, 'title_stream_length': 12,\n",
    "       'body_bm25': 105, 'body_idf': 15, 'body_vsm': 100, 'body_covered_query_term_number': 0, 'body_query_term_ratio': 5, 'body_stream_length': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_indices = [108, 18, 103, 3, 8, 13, 125, 126, 107, 17, 102, 2, 7, 12, 105, 15, 100, 0, 5, 10]\n",
    "feature_indices = [108, 18, 103, 3, 8, 126, 107, 17, 102, 2, 7, 12, 105, 15, 100, 0, 5, 10]\n",
    "feature_indices = [108, 103, 3, 8, 107, 102, 2, 7, 105, 100, 0, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "X_train, y_train = load_data(path=\"../../../data/MSLR-WEB10K/Fold1/train.txt\", nrows=20000, feature_indices=feature_indices)\n",
    "#X_test, y_test = load_data(path=\"../../../data/MSLR-WEB10K/Fold1/test.txt\", nrows=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label counts (array([0., 1., 2., 3., 4.]), array([11633,  5644,  2354,   267,   102]))\n"
     ]
    }
   ],
   "source": [
    "print('label counts', np.unique(y_train, return_counts=True))\n",
    "n_samples_per_class = np.unique(y_train, return_counts=True)[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(510, 12) (510,)\n",
      "label counts:  (array([0., 1., 2., 3., 4.]), array([102, 102, 102, 102, 102]))\n"
     ]
    }
   ],
   "source": [
    "# balance dataset\n",
    "indices = []\n",
    "for label in range(5):\n",
    "    indices.append(list(np.random.choice(np.where(y_train == label)[0], n_samples_per_class, replace=False)))\n",
    "    \n",
    "indices = np.array(indices).flatten()\n",
    "\n",
    "X_cut = X_train[indices, :]\n",
    "y_cut = y_train[indices]\n",
    "\n",
    "print(X_cut.shape, y_cut.shape)\n",
    "print('label counts: ', np.unique(y_cut, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trans = torch.zeros(len(y_cut), 5)\n",
    "for i, label in enumerate(y_cut):\n",
    "    y_trans[i, 0:int(label)+1] = 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(n_features=len(feature_indices), n_hidden=10, load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define optimizer and loss fct\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 4.772689  [   32/  510]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 4.251239  [   32/  510]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 4.534404  [   32/  510]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 5.540410  [   32/  510]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 4.968125  [   32/  510]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 5.109271  [   32/  510]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 4.689440  [   32/  510]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 5.048555  [   32/  510]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 5.334731  [   32/  510]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 4.624043  [   32/  510]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 4.879647  [   32/  510]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 4.397687  [   32/  510]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 4.934406  [   32/  510]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 4.886223  [   32/  510]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 5.155272  [   32/  510]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 4.893486  [   32/  510]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 4.237883  [   32/  510]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 4.245594  [   32/  510]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 5.288335  [   32/  510]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 4.607224  [   32/  510]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 4.271894  [   32/  510]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 3.772121  [   32/  510]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 4.711129  [   32/  510]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 4.861760  [   32/  510]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 4.395529  [   32/  510]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 4.754199  [   32/  510]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 4.828880  [   32/  510]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 4.608561  [   32/  510]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 4.377643  [   32/  510]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 4.015539  [   32/  510]\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 4.433131  [   32/  510]\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 4.549497  [   32/  510]\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 5.239724  [   32/  510]\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 5.362824  [   32/  510]\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 4.715718  [   32/  510]\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 5.028546  [   32/  510]\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 4.462600  [   32/  510]\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 4.979763  [   32/  510]\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 4.764531  [   32/  510]\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 5.148880  [   32/  510]\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 4.172137  [   32/  510]\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 4.497987  [   32/  510]\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 4.870024  [   32/  510]\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 5.341228  [   32/  510]\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 5.150982  [   32/  510]\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 4.939524  [   32/  510]\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 4.976377  [   32/  510]\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 4.248692  [   32/  510]\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 4.731415  [   32/  510]\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 4.652479  [   32/  510]\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 4.556927  [   32/  510]\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 4.930604  [   32/  510]\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 5.072945  [   32/  510]\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 5.612692  [   32/  510]\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 4.128994  [   32/  510]\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 4.379924  [   32/  510]\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 4.284283  [   32/  510]\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 4.916338  [   32/  510]\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 4.862967  [   32/  510]\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 5.131537  [   32/  510]\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 4.599108  [   32/  510]\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 4.589633  [   32/  510]\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 4.778487  [   32/  510]\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 4.836006  [   32/  510]\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 4.751856  [   32/  510]\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 5.247491  [   32/  510]\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 3.691007  [   32/  510]\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 4.317663  [   32/  510]\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 5.296193  [   32/  510]\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 4.590767  [   32/  510]\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 4.936690  [   32/  510]\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 4.641793  [   32/  510]\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 4.467033  [   32/  510]\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 4.970884  [   32/  510]\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 5.400185  [   32/  510]\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 4.919605  [   32/  510]\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 4.277015  [   32/  510]\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 4.697206  [   32/  510]\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 4.585669  [   32/  510]\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 4.794165  [   32/  510]\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 5.145905  [   32/  510]\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 5.572579  [   32/  510]\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 4.875634  [   32/  510]\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 4.662270  [   32/  510]\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 4.501047  [   32/  510]\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 4.692530  [   32/  510]\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 4.653129  [   32/  510]\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 5.347054  [   32/  510]\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 4.642055  [   32/  510]\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 4.772693  [   32/  510]\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 4.823346  [   32/  510]\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 4.453113  [   32/  510]\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 5.031446  [   32/  510]\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 5.471896  [   32/  510]\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 4.868551  [   32/  510]\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 5.360574  [   32/  510]\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 4.924343  [   32/  510]\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 5.016446  [   32/  510]\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 4.443048  [   32/  510]\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 4.482607  [   32/  510]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loss = model.train_loop(torch.Tensor(X_cut), y_trans, loss, optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.2\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.evaluate(torch.Tensor(X_cut))\n",
    "print('accuracy: ', np.sum(np.array(y_pred) == np.array(y_cut)) / len(y_cut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True]), array([408, 102]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array(y_pred) == np.array(y_cut), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), array([510]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.unique(y_pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49999999999999994"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg = ndcg_score(y_cut.reshape(1, -1), y_pred.reshape(1, -1), k=20)\n",
    "ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save mdoel\n",
    "model.save(\"../models/nn.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise SVM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(510,)\n",
      "n_samples after pairwise transform  208080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annavollweiter/Documents/Uni/MSE/mse_project_team05/.venv/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(y_cut.shape)\n",
    "svm = RankSVM(load=False)\n",
    "svm.fit(X_cut, y_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02883423,  0.21695335, -0.26878167, -0.21214111,  0.02746978,\n",
       "        -0.78000491,  0.318019  ,  0.03747703,  0.00310763,  0.30371981,\n",
       "        -0.18882598, -0.16140359]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06808976, -0.09105113,  0.1003668 , -0.22147471,  0.06969051,\n",
       "       -0.03106701, -0.19279618, -0.52288695, -0.31845989,  0.25137458,\n",
       "        0.25528594,  0.        , -0.63044159,  0.09289614,  0.22631097,\n",
       "       -0.32727074,  0.        , -0.03801811,  0.        ,  0.119675  ,\n",
       "        0.        ,  0.0861692 , -0.13905482, -0.08857685, -0.05738022,\n",
       "       -0.24088161, -0.14889187,  0.27764925, -0.24410914, -0.12673423,\n",
       "       -0.2273243 ,  0.05447276, -0.24048315,  0.        , -0.11764717,\n",
       "       -0.13684448,  0.        , -0.32744117,  0.15710861, -0.27015591,\n",
       "       -0.38543318,  0.03556509, -0.40656977,  0.12848449,  0.53172015,\n",
       "        0.01824406,  0.03129232,  0.07167787,  0.41373617, -0.39445903,\n",
       "        0.26874884, -0.18107534, -0.00544969, -0.01074187,  0.25044873,\n",
       "        0.49648469,  0.        , -0.14226012,  0.05542743,  0.10216847,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.55837999,\n",
       "       -0.16905079,  0.25386844, -0.23018993, -0.51728109, -0.16149281,\n",
       "        0.        , -0.04053661, -0.08756745, -0.46136782,  0.0672742 ,\n",
       "        0.06734511,  0.15566854, -0.23323984,  0.        , -0.37884603,\n",
       "        0.18763487,  0.04476023, -0.02135995, -0.17589624, -0.34787916,\n",
       "        0.31439172, -0.42530622, -0.29851983,  0.19259655, -0.04460461,\n",
       "        0.        ,  0.39889464,  0.62607228,  0.        ,  0.        ,\n",
       "       -0.19347686,  0.09989262, -0.17068473,  0.        , -0.17923045,\n",
       "        0.24873167,  0.16888717,  0.49818638,  0.08970267,  0.2858901 ,\n",
       "        0.18261198,  0.0345007 ,  0.08582737,  0.0412167 , -0.3204487 ,\n",
       "        0.88634175,  0.5152014 ,  0.32411556,  0.31778637,  0.0345369 ,\n",
       "        0.38088817, -0.11376108,  0.30806533,  0.3039811 ,  0.18411932,\n",
       "       -0.11829811,  0.23999605,  0.05805754, -0.0827882 ,  0.20520666,\n",
       "       -0.09951392, -0.0097348 ,  0.53144571, -0.18459008,  0.22087837,\n",
       "        0.        , -0.1113466 , -0.14475304,  0.63959874,  0.        ,\n",
       "       -0.01485497, -0.66256384,  0.14076081,  0.0860769 ,  0.06412406,\n",
       "       -0.20074743,  0.34752955, -0.15096489, -0.33732795, -0.24358477,\n",
       "        0.00717579, -0.13314035, -0.26829201,  0.25501681, -0.12563547,\n",
       "        0.55868774,  0.6157012 ,  0.37631676,  0.18152141,  0.        ,\n",
       "       -0.08089919,  0.49791666, -0.37148674,  0.16971799, -0.35512271,\n",
       "       -0.31465606,  0.02911079,  0.        ,  0.33252793,  0.67958855,\n",
       "        0.01061962, -0.10375207,  0.16217605, -0.2568851 ,  0.30965618,\n",
       "        0.36673822,  0.26710931, -0.42043517,  0.13199849,  0.54333337,\n",
       "        0.1195059 ,  0.15675342, -0.15878941, -0.23931102,  0.44142751,\n",
       "        0.        ,  0.46668782,  0.        ,  0.13823154,  0.        ,\n",
       "        0.1372944 ,  0.25779316, -0.11170735, -0.18030264,  0.09073222,\n",
       "       -0.22856083,  0.33981665,  0.14694044,  0.2042623 ,  0.3957985 ,\n",
       "        0.19083228, -0.26373613, -0.07397261, -0.02243471,  0.03673936,\n",
       "        0.42207843, -0.25137657,  0.0686332 , -0.08112897,  0.24889099,\n",
       "       -0.22075651,  0.15044578, -0.14630314,  0.30733071, -0.0176544 ,\n",
       "        0.06624659, -0.02966136, -0.33044933,  0.        ,  0.00791148,\n",
       "        0.15899689,  0.28273649, -0.11579227, -0.0415734 , -0.19106519,\n",
       "        0.19736046, -0.03200407,  0.57512212,  0.26246384,  0.05248554,\n",
       "        0.09624951,  0.20385914,  0.15195263, -0.15632419,  0.42078616,\n",
       "       -0.1954339 ,  0.58183276,  0.09822415,  0.43879749, -0.24567639,\n",
       "        0.68344302,  0.2032292 ,  0.01984776,  0.43060488,  0.3594505 ,\n",
       "        0.52206971,  0.17227201,  0.01144492,  0.27923529,  0.        ,\n",
       "        0.16267838,  0.2407118 , -0.05396536,  0.46029213,  0.61504019,\n",
       "       -0.06058331,  0.03304591, -0.10230875,  0.41089171,  0.47176321,\n",
       "        0.30110387,  0.07596878, -0.27645018,  0.22538545,  0.76134137,\n",
       "        0.26516933,  0.22301095,  0.55368194,  0.02628425, -0.31996779,\n",
       "        0.01546903, -0.01085049, -0.29805   ,  0.91952482,  0.        ,\n",
       "        0.15562015,  0.25691836,  0.0474018 , -0.17320276,  0.55252125,\n",
       "        0.3182755 ,  0.05445398,  0.31808743,  0.10455349, -0.17193888,\n",
       "        0.13448912, -0.04235817,  0.30002874,  0.42403242,  0.49824277,\n",
       "        0.83708443,  0.11712107, -0.09413901,  0.61794961,  0.19732082,\n",
       "        0.43546305, -0.2345828 ,  0.3740294 , -0.10739013, -0.06463383,\n",
       "       -0.24520039,  0.        ,  0.1284728 , -0.19597369,  0.12840801,\n",
       "       -0.24604233,  0.46856764, -0.00491021, -0.56589611,  0.65272775,\n",
       "        0.71508829, -0.17794636,  0.05226531, -0.10994366, -0.01808005,\n",
       "        0.06884477, -0.39473698,  0.31859361,  0.        ,  0.22457358,\n",
       "        0.        ,  0.10585627, -0.05768289,  0.40135079, -0.21254742,\n",
       "        0.08197079, -0.25138726,  0.        ,  0.06868107,  0.        ,\n",
       "       -0.15978001,  0.20060128,  0.29887401,  0.1003468 ,  0.03455608,\n",
       "        0.37889813,  0.42164391,  0.        ,  0.61132527, -0.25287208,\n",
       "        0.02342824,  0.        ,  0.36306947,  0.28886304,  0.22211345,\n",
       "        0.        ,  0.38067214,  0.2151522 ,  0.62298447,  0.36902999,\n",
       "       -0.09538361, -0.05749513,  0.4688323 ,  0.36902999, -0.19115508,\n",
       "       -0.01411322, -0.19981706,  0.        ,  0.        ,  0.        ,\n",
       "       -0.26019951, -0.1284778 , -0.16351524,  0.1936236 ,  0.04765411,\n",
       "        0.53570808,  0.93252144, -0.10563432,  0.3506203 , -0.21257381,\n",
       "        0.39009225,  0.17104729,  0.254856  ,  0.31655166, -0.30676696,\n",
       "        0.34161224,  0.05936717, -0.08016059,  0.42464453,  0.31880931,\n",
       "       -0.23737123,  0.20727561, -0.21573982, -0.22414339,  0.10718521,\n",
       "        0.02836149,  0.05530289,  0.29199168,  0.21210651,  0.31987748,\n",
       "       -0.52604442, -0.0465743 , -0.11933164,  0.44852532, -0.31201343,\n",
       "       -0.25590749,  0.13555251,  0.04952101, -0.11640018,  0.48585093,\n",
       "        0.55243563,  0.01217591,  0.44337614,  0.54585084,  0.1494133 ,\n",
       "        0.3506203 ,  0.31074198,  0.50065859, -0.02770496,  0.31953864,\n",
       "        0.30779333, -0.04412123,  0.54321835,  0.30190855,  0.        ,\n",
       "        0.31658998,  0.        , -0.44264437,  0.4019351 ,  0.2220583 ,\n",
       "        0.45578212,  0.08173039,  0.09352556,  0.2220583 ,  0.6195976 ,\n",
       "        0.2220583 ,  0.47717563,  0.21297923,  0.69958951,  0.        ,\n",
       "       -0.15352649,  0.69540925,  0.47720074,  0.11998486,  0.31658998,\n",
       "        0.29997071,  0.48683907,  0.33916602,  0.38859965,  0.47725707,\n",
       "        0.05312832,  0.74722364,  0.        ,  0.16461799,  0.26287729,\n",
       "        0.18161184,  0.40591472,  0.61259541,  0.26079085,  0.07901705,\n",
       "        0.55980022,  0.61602889, -0.24727272,  0.07461169,  0.05312832,\n",
       "       -0.37148674, -0.00688954,  0.31860747,  0.        ,  0.25701235,\n",
       "        0.6365048 ,  0.93034686,  0.45656089,  0.03270521,  0.49044959,\n",
       "        0.11998486,  1.10396827,  0.49003077,  0.49196703,  0.0994281 ,\n",
       "       -0.22204284,  0.37268822,  0.38859965,  0.28400534,  0.16790741,\n",
       "        0.6165315 , -0.07504192,  0.47689728,  0.69540925,  0.47775474,\n",
       "        0.        ,  0.05558291, -0.37148674,  0.23665282,  0.09965137,\n",
       "        0.30465056,  0.61953311,  0.25916328,  0.24945792,  0.02915734,\n",
       "        0.47721723,  0.25974602,  0.56281561,  0.38859965,  0.45656089,\n",
       "        0.22466434,  0.03270521,  0.64820152, -0.06336407,  0.69540925,\n",
       "        0.69540925,  0.93034686,  0.16790741,  0.23795345,  0.70059347,\n",
       "        0.77304508, -0.0306809 ,  0.27828716, -0.03822927,  0.29997071,\n",
       "        0.6331433 ,  0.33412947,  0.55866017,  0.24352752,  0.05312832])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = svm.predict(X_cut)\n",
    "svm.save(\"../models/ranksvm.pkl\")\n",
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mse_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
